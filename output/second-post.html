<!DOCTYPE html> <html lang=en > <meta charset=utf-8 > <title>Second Post - Part 2 - Thomas Dargent</title> <link href="/theme/css/main.css" rel=stylesheet > <link href="/theme/css/pygments-colorful.css" rel=stylesheet > <!-- Used for pelican_dynamic--> <nav> <h1><a href="/">Thomas Dargent<span class=subtitle > – Data Scientist</span></a></h1> <ul> <li><a href="/index.html">Home</a> <li><a href="/archives.html">Archive</a> <li><a href="/pages/cv.html">CV</a> <li><a href="/pages/about-me.html">About</a> </ul> </nav> <div class=container > <div class=article > <h1>Second Post - Part 2</h1> <time datetime="2017-11-30 12:40:00+01:00">30 novembre 2017</time> <p class='est'>Estimated reading time: 4 min.</p> <div style="display:none;">$$ \definecolor{w1}{RGB}{230,159,0} \definecolor{w2}{RGB}{86,180,233} \definecolor{w3}{RGB}{0,158,115} \definecolor{w4}{RGB}{204,121,167} \definecolor{w5}{RGB}{0,114,178} \definecolor{w6}{RGB}{213,94,0} \definecolor{w7}{RGB}{240,228,66} $$</div> <p>This is the second post of a series of posts. It will show series and an embedded youtube video.</p> <h1>Introduction</h1> <p>In the last few years, deep neural networks have dominated pattern recognition. They blew the previous state of the art out of the water for many computer vision tasks. Voice recognition is also moving that way.</p> <p>But despite the results, we have to wonder… why do they work so well?</p> <p>This post reviews some extremely remarkable results in applying deep neural networks to natural language processing (NLP). In doing so, I hope to make accessible one promising answer as to why deep neural networks work. I think it’s a very elegant perspective.</p> <h2>One Hidden Layer Neural Networks</h2> <p>A neural network with a hidden layer has universality: given enough hidden units, it can approximate any function. This is a frequently quoted – and even more frequently, misunderstood and applied – theorem.</p> <p>It’s true, essentially, because the hidden layer can be used as a lookup table.</p> <p>For simplicity, let’s consider a perceptron network. A perceptron is a very simple neuron that fires if it exceeds a certain threshold and doesn’t fire if it doesn’t reach that threshold. A perceptron network gets binary (0 and 1) inputs and gives binary outputs.</p> <p>Note that there are only a finite number of possible inputs. For each possible input, we can construct a neuron in the hidden layer that fires for that input,1 and only on that specific input. Then we can use the connections between that neuron and the output neurons to control the output in that specific case.<sup id=sf-second-post-1-back><a href=#sf-second-post-1 class=simple-footnote title="It isn’t only perceptron networks that have universality. Networks of sigmoid neurons (and other activation functions) are also universal: give enough hidden neurons, they can approximate any continuous function arbitrarily well. Seeing this is significantly trickier because you can’t just isolate inputs.">1</a></sup></p> <p><img alt="" src="https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/img/flowchart-PerceptronLookup.png"></p> <p>And so, it’s true that one hidden layer neural networks are universal. But there isn’t anything particularly impressive or exciting about that. Saying that your model can do the same thing as a lookup table isn’t a very strong argument for it. It just means it isn’t impossible for your model to do the task.</p> <p>Universality means that a network can fit to any training data you give it. It doesn’t mean that it will interpolate to new data points in a reasonable way.</p> <p>No, universality isn’t an explanation for why neural networks work so well. The real reason seems to be something much more subtle… And, to understand it, we’ll first need to understand some concrete results.</p> <h2>Word Embeddings</h2> <p>I’d like to start by tracing a particularly interesting strand of deep learning research: word embeddings. In my personal opinion, word embeddings are one of the most exciting area of research in deep learning at the moment, although they were originally introduced by Bengio, et al. more than a decade ago.<sup id=sf-second-post-2-back><a href=#sf-second-post-2 class=simple-footnote title="Word embeddings were originally developed in (Bengio et al, 2001; Bengio et al, 2003), a few years before the 2006 deep learning renewal, at a time when neural networks were out of fashion. The idea of distributed representations for symbols is even older, e.g. (Hinton 1986).">2</a></sup> Beyond that, I think they are one of the best places to gain intuition about why deep learning is so effective.</p> <p>A word embedding <span class=math>\(W: \text{words}\rightarrow \mathbb{R}^n\)</span> is a paramaterized function mapping words in some language to high-dimensional vectors (perhaps 200 to 500 dimensions). For example, we might find:</p> <p>W(‘‘cat")=(0.2, -0.4, 0.7, ...) !youtube(TI56sJ_ar4k)</p> <script>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = "center", indent = "0em", linebreak = "false"; if (false) { align = (screen.width < 768) ? "left" : align; indent = (screen.width < 768) ? "0em" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? "innerHTML" : "text")] = "MathJax.Hub.Config({" + " config: ['MMLorHTML.js']," + " TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," + " jax: ['input/TeX','input/MathML','output/HTML-CSS']," + " extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," + " displayAlign: '"+ align +"'," + " displayIndent: '"+ indent +"'," + " showMathMenu: true," + " messageStyle: 'normal'," + " tex2jax: { " + " inlineMath: [ ['\\\\(','\\\\)'] ], " + " displayMath: [ ['$$','$$'] ]," + " processEscapes: true," + " preview: 'TeX'," + " }, " + " 'HTML-CSS': { " + " availableFonts: ['STIX', 'TeX']," + " preferredFont: 'STIX'," + " styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," + " linebreaks: { automatic: "+ linebreak +", width: '90% container' }," + " }, " + "}); " + "if ('default' !== 'default') {" + "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" + "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" + "VARIANT['normal'].fonts.unshift('MathJax_default');" + "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" + "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" + "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" + "});" + "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" + "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" + "VARIANT['normal'].fonts.unshift('MathJax_default');" + "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" + "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" + "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" + "});" + "}"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); } </script><ol class=simple-footnotes><li id=sf-second-post-1>It isn’t only perceptron networks that have universality. Networks of sigmoid neurons (and other activation functions) are also universal: give enough hidden neurons, they can approximate any continuous function arbitrarily well. Seeing this is significantly trickier because you can’t just isolate inputs. <a href=#sf-second-post-1-back class=simple-footnote-back>↩</a><li id=sf-second-post-2>Word embeddings were originally developed in (Bengio et al, 2001; Bengio et al, 2003), a few years before the 2006 deep learning renewal, at a time when neural networks were out of fashion. The idea of distributed representations for symbols is even older, e.g. (Hinton 1986). <a href=#sf-second-post-2-back class=simple-footnote-back>↩</a></ol> <aside> <h1>Similar posts:</h1> <ul> <li><a href="/first-post.html">First Post - Part 1</a> <li><a href="/comb.html">Projet d'Optimisation Combinatoire</a> </ul> </aside> </div> <footer> Powered by <a href="http://getpelican.com/">Pelican</a>, "Moineau" theme by Thomas Dargent </footer> </div> <!--Used for pelican_dynamic--> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }}); </script> <script> var tables = Array.from(document.getElementsByTagName("table")); var imgs = Array.from(document.getElementsByTagName("img")); function center_elems(elems) { for (i = 0; i < elems.length; ++i) { var parent = elems[i].parentNode; var wrapper = document.createElement('div'); wrapper.className = "centered-container" parent.replaceChild(wrapper, elems[i]); wrapper.appendChild(elems[i]); } } center_elems(tables); center_elems(imgs); </script>